{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bb9be7-38c2-48ee-bcdc-4eb05588a2f2",
   "metadata": {},
   "source": [
    "# Unamortised VAE implementation\n",
    "\n",
    "We seek an API that trains an LVM given\n",
    "1. Model\n",
    "2. Loss function\n",
    "3. Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87b4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TPU_CHIPS_PER_HOST_BOUNDS\"] = \"1,1,1\"\n",
    "os.environ[\"TPU_HOST_BOUNDS\"] = \"1,1,1\"\n",
    "os.environ[\"TPU_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf93b37-c528-4c58-9744-82338f567851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import hlax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32926aa2-1b82-4aab-9a77-c9a4166fded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9354b5-1ef5-4f49-a9d5-083ecc97046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    For the generative model\n",
    "    p(x,z) = p(x|z) * p(z)\n",
    "    \"\"\"\n",
    "    dim_full: int\n",
    "    dim_latent: int = 20\n",
    "    \n",
    "    def setup(self):\n",
    "        self.logit = nn.Dense(self.dim_full, use_bias=True, name=\"mean\")\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, z):\n",
    "        x = nn.Dense(20)(z)\n",
    "        x = nn.elu(x)\n",
    "        logit_x = self.logit(x)\n",
    "        return logit_x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    two-layered encoder\n",
    "    \"\"\"\n",
    "    latent_dim: int\n",
    "    n_hidden: int = 100\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        z = nn.Dense(self.n_hidden)(x)\n",
    "        z = nn.elu(z)\n",
    "        z = nn.Dense(self.n_hidden)(z)\n",
    "        z = nn.elu(z)\n",
    "        mean_z = nn.Dense(self.latent_dim)(z)\n",
    "        logvar_z = nn.Dense(self.latent_dim)(z)\n",
    "        return mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f2d8b-c058-4e24-a31f-267fef5a3d7c",
   "metadata": {},
   "source": [
    "## Initialisiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12522df-a296-4351-ad4a-d9ebe7c743a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9a4940-cc8d-4834-98d3-8f857cf51e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup, test = hlax.datasets.load_fashion_mnist(n_train=1000, n_test=100)\n",
    "X_warmup = warmup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301f8566-383e-4876-9fe9-2d0b5c8ad2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn_vae = hlax.losses.iwae\n",
    "lossfn_hardem = hlax.losses.loss_hard_nmll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6d01ec-ef23-4990-8596-8e0dd7be44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_latent = 100\n",
    "num_obs, *dim_obs = X_warmup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f6c686-8980-4ebf-8e36-35375b02e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = hlax.models.UnamortisedVAEBern(dim_latent, dim_obs[0], Encoder, Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cbcbe5c9-4429-4b6f-9f20-69594aaf7760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        decoder: {\n",
       "            Dense_0: {\n",
       "                bias: (20,),\n",
       "                kernel: (100, 20),\n",
       "            },\n",
       "            mean: {\n",
       "                bias: (784,),\n",
       "                kernel: (20, 784),\n",
       "            },\n",
       "        },\n",
       "        encoder: {\n",
       "            Dense_0: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 784, 100),\n",
       "            },\n",
       "            Dense_1: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "            Dense_2: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "            Dense_3: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 200\n",
    "key_params_init, key_eps_init = jax.random.split(key)\n",
    "\n",
    "# Initialise model consider total number of observations.\n",
    "# We will then slice over the batches\n",
    "batch_init = jnp.ones((num_obs, *dim_obs))\n",
    "params = model_vae.init(key_params_init, batch_init, key_eps_init, num_samples=4)\n",
    "\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8b30dd-4774-4167-acc5-de03109a015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_batch, keys_vae = jax.random.split(key)\n",
    "batch_ixs = hlax.training.get_batch_train_ixs(key_batch, num_obs, batch_size)\n",
    "num_batches = len(batch_ixs)\n",
    "keys_vae = jax.random.split(keys_vae, num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c88b6f4-4616-4421-936e-34776833ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ix = batch_ixs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a5bb0-a592-44fd-823e-e84172016f64",
   "metadata": {},
   "source": [
    "## Param surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba714d1b-6dd4-47db-bde0-188f4eba10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distrax\n",
    "def iwae_bern(key, params, apply_fn, X_batch):\n",
    "    \"\"\"\n",
    "    Importance-weighted marginal log-likelihood for\n",
    "    a Bernoulli decoder\n",
    "    \"\"\"\n",
    "    batch_size = len(X_batch)\n",
    "\n",
    "    # keys = jax.random.split(key, batch_size)\n",
    "    # encode_decode = jax.vmap(apply_fn, (None, 0, 0))\n",
    "    # encode_decode = encode_decode(params, X_batch, keys)\n",
    "    encode_decode = apply_fn(params, X_batch, key)\n",
    "    z, (mean_z, logvar_z), logit_mean_x = encode_decode\n",
    "    _, num_is_samples, dim_latent = z.shape\n",
    "\n",
    "    std_z = jnp.exp(logvar_z / 2)\n",
    "    \n",
    "    dist_prior = distrax.MultivariateNormalDiag(jnp.zeros(dim_latent),\n",
    "                                                jnp.ones(dim_latent))\n",
    "    dist_decoder = distrax.Bernoulli(logits=logit_mean_x)\n",
    "    dist_posterior = distrax.Normal(mean_z[None, ...], std_z[None, ...])\n",
    "\n",
    "    log_prob_z_prior = dist_prior.log_prob(z)\n",
    "    log_prob_x = dist_decoder.log_prob(X_batch).sum(axis=-1)\n",
    "    log_prob_z_post = dist_posterior.log_prob(z).sum(axis=-1)\n",
    "\n",
    "    log_prob = log_prob_z_prior + log_prob_x - log_prob_z_post\n",
    "\n",
    "    # negative Importance-weighted marginal log-likelihood\n",
    "    niwmll = -jax.nn.logsumexp(log_prob, axis=-1, b=1/num_is_samples).mean()\n",
    "    return niwmll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a9f5252-c153-4f7e-aad2-bb32928c7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from functools import partial\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax.training.train_state import TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c6e81f44-2e10-45f3-848d-a6feff9cb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_batch_adam_params_encoder(opt_state, ixs):\n",
    "    \"\"\"\n",
    "    Get mu and nu optimiser parameters\n",
    "    \"\"\"\n",
    "    encoder_sub_mu = state.opt_state[0].mu[\"params\"][\"encoder\"]\n",
    "    encoder_sub_nu = state.opt_state[0].nu[\"params\"][\"encoder\"]\n",
    "    \n",
    "    encoder_sub_mu = jax.tree_map(lambda x: x[ixs], encoder_sub_mu)\n",
    "    encoder_sub_nu = jax.tree_map(lambda x: x[ixs], encoder_sub_nu)\n",
    "    \n",
    "    return encoder_sub_mu, encoder_sub_nu\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def create_batch_adam_params(opt_state, ixs):\n",
    "    mu_sub, nu_sub = get_batch_adam_params_encoder(opt_state, ixs)\n",
    "    \n",
    "    opt_state_batch = opt_state[0]\n",
    "    mu_params = unfreeze(opt_state_batch.mu)\n",
    "    nu_params = unfreeze(opt_state_batch.nu)\n",
    "    \n",
    "    # Replace encoder opt params with indexed params\n",
    "    mu_params[\"params\"][\"encoder\"] = mu_sub\n",
    "    nu_params[\"params\"][\"encoder\"] = nu_sub\n",
    "    \n",
    "    opt_state_batch = opt_state_batch._replace(\n",
    "        mu=freeze(mu_params),\n",
    "        nu=freeze(nu_params)\n",
    "    )\n",
    "    \n",
    "    opt_state_batch = (\n",
    "        opt_state_batch,\n",
    "    ) + opt_state[1:]\n",
    "    \n",
    "    return opt_state_batch\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pytree(pytree, pytree_subset, ixs):\n",
    "    \"\"\"\n",
    "    Update the subset of a pytree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pytree:\n",
    "        Target pytree\n",
    "    pytree_subset\n",
    "        Values of the pytree to update\n",
    "    ixs: DeviceArray\n",
    "        Indices mapping from the subset to the\n",
    "        full pytree\n",
    "    \"\"\"\n",
    "    pytree_update = jax.tree_map(\n",
    "        lambda vfull, vsub: vfull.at[ixs].set(vsub),\n",
    "        pytree, pytree_subset\n",
    "    )\n",
    "    return pytree_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c8b6a0e8-aeed-4f54-a63c-ab7b1bc49914",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "tx = optax.adam(1e-5)\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=partial(model_vae.apply, num_samples=num_samples),\n",
    "    params=params,\n",
    "    tx=tx,\n",
    ")\n",
    "\n",
    "# 1. Obtain batch of encoder params\n",
    "params_batch_encoder = jax.tree_map(lambda x: x[batch_ix], state.params[\"params\"][\"encoder\"])\n",
    "\n",
    "\n",
    "# 2. Construct encoder/decoder set of params\n",
    "params_batch = freeze({\n",
    "    \"params\": {\n",
    "        \"encoder\": params_batch_encoder,\n",
    "        \"decoder\": params[\"params\"][\"decoder\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "# 3. Create batch of optimisation-state params\n",
    "opt_state_batch = create_batch_adam_params(state.opt_state, batch_ix)\n",
    "\n",
    "\n",
    "# 4. Create batch state\n",
    "state_batch = TrainState(\n",
    "    step=state.step,\n",
    "    apply_fn=state.apply_fn,\n",
    "    tx=state.tx,\n",
    "    params=params_batch,\n",
    "    opt_state=opt_state_batch,\n",
    ")\n",
    "\n",
    "# 5. Obtain updated batch state\n",
    "X_batch = X_warmup[batch_ix]\n",
    "loss_valgrad = jax.value_and_grad(iwae_bern, 1)\n",
    "loss, grads_batch = loss_valgrad(key, state_batch.params, state_batch.apply_fn, X_batch)\n",
    "new_state_batch = state_batch.apply_gradients(grads=grads_batch)\n",
    "\n",
    "\n",
    "# 6. Update and reconstruct decoder params and batch of encoder params\n",
    "params_encoder_update = unfreeze(params[\"params\"][\"encoder\"])\n",
    "params_batch_encoder_update = unfreeze(state_batch.params[\"params\"][\"encoder\"])\n",
    "params_encoder_update = update_pytree(params_encoder_update, params_batch_encoder_update, batch_ix)\n",
    "\n",
    "params_decoder_update = unfreeze(state_batch.params[\"params\"][\"decoder\"])\n",
    "\n",
    "params_update = {\n",
    "    \"params\": {\n",
    "        \"encoder\": params_encoder_update,\n",
    "        \"decoder\": params_decoder_update,\n",
    "    }\n",
    "}\n",
    "\n",
    "params_update = freeze(params_update)\n",
    "\n",
    "\n",
    "# 7. Update and reconstruct the optimiser state\n",
    "mu_state = unfreeze(state.opt_state[0].mu)\n",
    "nu_state = unfreeze(state.opt_state[0].nu)\n",
    "\n",
    "# Update decoder optimisation-state params\n",
    "mu_state[\"params\"][\"decoder\"] = new_state_batch.opt_state[0].mu[\"params\"][\"decoder\"]\n",
    "nu_state[\"params\"][\"decoder\"] = new_state_batch.opt_state[0].nu[\"params\"][\"decoder\"]\n",
    "\n",
    "# Update encoder optimisation-state params\n",
    "batch_mu_encoder = unfreeze(new_state_batch.opt_state[0].mu[\"params\"][\"encoder\"])\n",
    "mu_state[\"params\"][\"encoder\"] = update_pytree(mu_state[\"params\"][\"encoder\"], batch_mu_encoder, batch_ix)\n",
    "\n",
    "batch_nu_encoder = unfreeze(new_state_batch.opt_state[0].nu[\"params\"][\"encoder\"])\n",
    "nu_state[\"params\"][\"encoder\"] = update_pytree(nu_state[\"params\"][\"encoder\"], batch_nu_encoder, batch_ix)\n",
    "\n",
    "mu_state = freeze(mu_state)\n",
    "nu_state = freeze(nu_state)\n",
    "\n",
    "opt_state_new = (\n",
    "    state.opt_state[0]._replace(\n",
    "        mu=mu_state,\n",
    "        nu=nu_state,\n",
    "    ),\n",
    ") + state.opt_state[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b1f86363-9595-4691-b52d-9f05927e7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def reconstruct_full_adam_params(opt_state, opt_state_sub, ixs):\n",
    "    mu_sub, nu_sub = get_batch_adam_params_encoder(opt_state_sub, ixs)\n",
    "\n",
    "    # mu_update = jax.tree_map(lambda x: x.at[ixs].set(mu_sub), opt_state[0].mu)\n",
    "    # nu_update = jax.tree_map(lambda x: x.at[ixs].set(nu_sub), opt_state[0].nu)\n",
    "    \n",
    "    import ipdb; ipdb.set_trace()\n",
    "    mu_update = jax.tree_map(lambda pfull, psub: pfull.at[ix].set(psub), opt_state[0].mu, mu_sub)\n",
    "    nu_update = jax.tree_map(lambda pfull, psub: pfull.at[ix].set(psub), opt_state[0].nu, nu_sub)\n",
    "\n",
    "    opt_latent_state_new = (\n",
    "        opt_latent_state[0]._replace(\n",
    "            mu=mu_update,\n",
    "            nu=nu_update\n",
    "        ),\n",
    "    ) + opt_latent_state[1:]\n",
    "\n",
    "    opt_state_new = (opt_latent_state_new, opt_params_update)\n",
    "    return opt_state_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
