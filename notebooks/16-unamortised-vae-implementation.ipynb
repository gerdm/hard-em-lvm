{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bb9be7-38c2-48ee-bcdc-4eb05588a2f2",
   "metadata": {},
   "source": [
    "# Unamortised VAE implementation\n",
    "\n",
    "We seek an API that trains an LVM given\n",
    "1. Model\n",
    "2. Loss function\n",
    "3. Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87b4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TPU_CHIPS_PER_HOST_BOUNDS\"] = \"1,1,1\"\n",
    "os.environ[\"TPU_HOST_BOUNDS\"] = \"1,1,1\"\n",
    "os.environ[\"TPU_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf93b37-c528-4c58-9744-82338f567851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import hlax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff66b24-9fb2-4fc6-b9f4-cdbb3b8acabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9354b5-1ef5-4f49-a9d5-083ecc97046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    For the generative model\n",
    "    p(x,z) = p(x|z) * p(z)\n",
    "    \"\"\"\n",
    "    dim_full: int\n",
    "    dim_latent: int = 20\n",
    "    \n",
    "    def setup(self):\n",
    "        self.logit = nn.Dense(self.dim_full, use_bias=True, name=\"mean\")\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, z):\n",
    "        x = nn.Dense(20)(z)\n",
    "        x = nn.elu(x)\n",
    "        logit_x = self.logit(x)\n",
    "        return logit_x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    two-layered encoder\n",
    "    \"\"\"\n",
    "    latent_dim: int\n",
    "    n_hidden: int = 100\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        z = nn.Dense(self.n_hidden)(x)\n",
    "        z = nn.elu(z)\n",
    "        z = nn.Dense(self.n_hidden)(z)\n",
    "        z = nn.elu(z)\n",
    "        mean_z = nn.Dense(self.latent_dim)(z)\n",
    "        logvar_z = nn.Dense(self.latent_dim)(z)\n",
    "        return mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f2d8b-c058-4e24-a31f-267fef5a3d7c",
   "metadata": {},
   "source": [
    "## Initialisiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8753d68-d5d3-4cb2-9a6f-6946aad9444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        decoder: {\n",
       "            Dense_0: {\n",
       "                bias: (20,),\n",
       "                kernel: (100, 20),\n",
       "            },\n",
       "            mean: {\n",
       "                bias: (784,),\n",
       "                kernel: (20, 784),\n",
       "            },\n",
       "        },\n",
       "        encoder: {\n",
       "            Dense_0: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 784, 100),\n",
       "            },\n",
       "            Dense_1: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "            Dense_2: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "            Dense_3: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 200\n",
    "key = jax.random.PRNGKey(314)\n",
    "warmup, test = hlax.datasets.load_fashion_mnist(n_train=1000, n_test=100)\n",
    "X_warmup = warmup[0]\n",
    "\n",
    "lossfn_vae = hlax.losses.iwae_bern\n",
    "lossfn_hardem = hlax.losses.hard_nmll_bern\n",
    "\n",
    "dim_latent = 50\n",
    "num_obs, *dim_obs = X_warmup.shape\n",
    "model_vae = hlax.models.UnamortisedVAEBern(dim_latent, dim_obs[0], Encoder, Decoder)\n",
    "\n",
    "\n",
    "key, key_params_init, key_eps_init = jax.random.split(key, 3)\n",
    "# Initialise model consider total number of observations.\n",
    "# We will then slice over the batches\n",
    "batch_init = jnp.ones((num_obs, *dim_obs))\n",
    "params = model_vae.init(key_params_init, batch_init, key_eps_init, num_samples=4)\n",
    "\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8b30dd-4774-4167-acc5-de03109a015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_batch, keys_vae = jax.random.split(key)\n",
    "batch_ixs = hlax.training.get_batch_train_ixs(key_batch, num_obs, batch_size)\n",
    "num_batches = len(batch_ixs)\n",
    "keys_vae = jax.random.split(keys_vae, num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a5bb0-a592-44fd-823e-e84172016f64",
   "metadata": {},
   "source": [
    "## Param surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9f5252-c153-4f7e-aad2-bb32928c7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from functools import partial\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax.training.train_state import TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba714d1b-6dd4-47db-bde0-188f4eba10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distrax\n",
    "def iwae_bern(key, params, apply_fn, X_batch):\n",
    "    \"\"\"\n",
    "    Importance-weighted marginal log-likelihood for\n",
    "    a Bernoulli decoder\n",
    "    \"\"\"\n",
    "    batch_size = len(X_batch)\n",
    "\n",
    "    # keys = jax.random.split(key, batch_size)\n",
    "    # encode_decode = jax.vmap(apply_fn, (None, 0, 0))\n",
    "    # encode_decode = encode_decode(params, X_batch, keys)\n",
    "    encode_decode = apply_fn(params, X_batch, key)\n",
    "    z, (mean_z, logvar_z), logit_mean_x = encode_decode\n",
    "    _, num_is_samples, dim_latent = z.shape\n",
    "\n",
    "    std_z = jnp.exp(logvar_z / 2)\n",
    "    \n",
    "    dist_prior = distrax.MultivariateNormalDiag(jnp.zeros(dim_latent),\n",
    "                                                jnp.ones(dim_latent))\n",
    "    dist_decoder = distrax.Bernoulli(logits=logit_mean_x)\n",
    "    dist_posterior = distrax.Normal(mean_z[None, ...], std_z[None, ...])\n",
    "\n",
    "    log_prob_z_prior = dist_prior.log_prob(z)\n",
    "    log_prob_x = dist_decoder.log_prob(X_batch).sum(axis=-1)\n",
    "    log_prob_z_post = dist_posterior.log_prob(z).sum(axis=-1)\n",
    "\n",
    "    log_prob = log_prob_z_prior + log_prob_x - log_prob_z_post\n",
    "\n",
    "    # negative Importance-weighted marginal log-likelihood\n",
    "    niwmll = -jax.nn.logsumexp(log_prob, axis=-1, b=1/num_is_samples).mean()\n",
    "    return niwmll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e81f44-2e10-45f3-848d-a6feff9cb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_batch_adam_params_encoder(opt_state, ixs):\n",
    "    \"\"\"\n",
    "    Get mu and nu optimiser parameters\n",
    "    \"\"\"\n",
    "    encoder_sub_mu = opt_state[0].mu[\"params\"][\"encoder\"]\n",
    "    encoder_sub_nu = opt_state[0].nu[\"params\"][\"encoder\"]\n",
    "    \n",
    "    encoder_sub_mu = jax.tree_map(lambda x: x[ixs], encoder_sub_mu)\n",
    "    encoder_sub_nu = jax.tree_map(lambda x: x[ixs], encoder_sub_nu)\n",
    "    \n",
    "    return encoder_sub_mu, encoder_sub_nu\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def create_batch_adam_params(opt_state, ixs):\n",
    "    mu_sub, nu_sub = get_batch_adam_params_encoder(opt_state, ixs)\n",
    "    \n",
    "    opt_state_batch = opt_state[0]\n",
    "    mu_params = unfreeze(opt_state_batch.mu)\n",
    "    nu_params = unfreeze(opt_state_batch.nu)\n",
    "    \n",
    "    # Replace encoder opt params with indexed params\n",
    "    mu_params[\"params\"][\"encoder\"] = mu_sub\n",
    "    nu_params[\"params\"][\"encoder\"] = nu_sub\n",
    "    \n",
    "    opt_state_batch = opt_state_batch._replace(\n",
    "        mu=freeze(mu_params),\n",
    "        nu=freeze(nu_params)\n",
    "    )\n",
    "    \n",
    "    opt_state_batch = (\n",
    "        opt_state_batch,\n",
    "    ) + opt_state[1:]\n",
    "    \n",
    "    return opt_state_batch\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pytree(pytree, pytree_subset, ixs):\n",
    "    \"\"\"\n",
    "    Update the subset of a pytree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pytree:\n",
    "        Target pytree\n",
    "    pytree_subset\n",
    "        Values of the pytree to update\n",
    "    ixs: DeviceArray\n",
    "        Indices mapping from the subset to the\n",
    "        full pytree\n",
    "    \"\"\"\n",
    "    pytree_update = jax.tree_map(\n",
    "        lambda vfull, vsub: vfull.at[ixs].set(vsub),\n",
    "        pytree, pytree_subset\n",
    "    )\n",
    "    return pytree_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f22e90-057f-450d-99ed-3081b623b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "tx = optax.adam(1e-3)\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=partial(model_vae.apply, num_samples=num_samples),\n",
    "    params=params,\n",
    "    tx=tx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c88b6f4-4616-4421-936e-34776833ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ix = batch_ixs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1accc674-34e7-4078-93a2-d36a94029bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def create_batch_state(state, ixs):\n",
    "    \"\"\"\n",
    "    Create a batch of the unamortised TrainStep\n",
    "    \"\"\"\n",
    "    params_batch_encoder = jax.tree_map(lambda x: x[ixs], state.params[\"params\"][\"decoder\"])\n",
    "    params_batch = freeze({\n",
    "        \"params\": {\n",
    "            \"encoder\": params_batch_encoder,\n",
    "            \"decoder\": state.params[\"params\"][\"decoder\"]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    opt_state_batch = create_batch_adam_params(state.opt_state, ixs)\n",
    "    \n",
    "    state_batch = TrainState(\n",
    "        step=state.step,\n",
    "        apply_fn=state.apply_fn,\n",
    "        tx=state.tx,\n",
    "        params=params_batch,\n",
    "        opt_state=opt_state_batch,\n",
    "    )\n",
    "    \n",
    "    return state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8892130-4d68-449e-8f02-f49c09851c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1801633/1852132205.py\u001b[0m(14)\u001b[0;36miwae_bern\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m    \u001b[0mencode_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m    \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_mean_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m    \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_is_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    }
   ],
   "source": [
    "# 1. Obtain batch of encoder params\n",
    "params_batch_encoder = jax.tree_map(lambda x: x[batch_ix], state.params[\"params\"][\"encoder\"])\n",
    "\n",
    "\n",
    "# 2. Construct encoder/decoder set of params\n",
    "params_batch = freeze({\n",
    "    \"params\": {\n",
    "        \"encoder\": params_batch_encoder,\n",
    "        \"decoder\": state.params[\"params\"][\"decoder\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "# 3. Create batch of optimisation-state params\n",
    "opt_state_batch = create_batch_adam_params(state.opt_state, batch_ix)\n",
    "\n",
    "\n",
    "# 4. Create batch state\n",
    "state_batch = TrainState(\n",
    "    step=state.step,\n",
    "    apply_fn=state.apply_fn,\n",
    "    tx=state.tx,\n",
    "    params=params_batch,\n",
    "    opt_state=opt_state_batch,\n",
    ")\n",
    "\n",
    "# 5. Obtain updated batch state\n",
    "X_batch = X_warmup[batch_ix]\n",
    "loss_valgrad = jax.value_and_grad(iwae_bern, 1)\n",
    "loss, grads_batch = loss_valgrad(key, state_batch.params, state_batch.apply_fn, X_batch)\n",
    "new_state_batch = state_batch.apply_gradients(grads=grads_batch)\n",
    "\n",
    "\n",
    "# 6. Update and reconstruct decoder params and batch of encoder params\n",
    "params_encoder_update = unfreeze(state.params[\"params\"][\"encoder\"])\n",
    "params_batch_encoder_update = unfreeze(new_state_batch.params[\"params\"][\"encoder\"])\n",
    "params_encoder_update = update_pytree(params_encoder_update, params_batch_encoder_update, batch_ix)\n",
    "\n",
    "params_decoder_update = unfreeze(new_state_batch.params[\"params\"][\"decoder\"])\n",
    "\n",
    "params_update = {\n",
    "    \"params\": {\n",
    "        \"encoder\": params_encoder_update,\n",
    "        \"decoder\": params_decoder_update,\n",
    "    }\n",
    "}\n",
    "\n",
    "params_update = freeze(params_update)\n",
    "\n",
    "\n",
    "# 7. Update and reconstruct the optimiser state\n",
    "mu_state = unfreeze(state.opt_state[0].mu)\n",
    "nu_state = unfreeze(state.opt_state[0].nu)\n",
    "\n",
    "\n",
    "# Update decoder optimisation-state params\n",
    "mu_state[\"params\"][\"decoder\"] = new_state_batch.opt_state[0].mu[\"params\"][\"decoder\"]\n",
    "nu_state[\"params\"][\"decoder\"] = new_state_batch.opt_state[0].nu[\"params\"][\"decoder\"]\n",
    "\n",
    "# Update encoder optimisation-state params\n",
    "batch_mu_encoder = unfreeze(new_state_batch.opt_state[0].mu[\"params\"][\"encoder\"])\n",
    "mu_state[\"params\"][\"encoder\"] = update_pytree(mu_state[\"params\"][\"encoder\"], batch_mu_encoder, batch_ix)\n",
    "\n",
    "batch_nu_encoder = unfreeze(new_state_batch.opt_state[0].nu[\"params\"][\"encoder\"])\n",
    "nu_state[\"params\"][\"encoder\"] = update_pytree(nu_state[\"params\"][\"encoder\"], batch_nu_encoder, batch_ix)\n",
    "\n",
    "mu_state = freeze(mu_state)\n",
    "nu_state = freeze(nu_state)\n",
    "\n",
    "opt_state_update = (\n",
    "    state.opt_state[0]._replace(\n",
    "        mu=mu_state,\n",
    "        nu=nu_state,\n",
    "    ),\n",
    ") + state.opt_state[1:]\n",
    "\n",
    "\n",
    "# 8. Obtain new state\n",
    "state = TrainState(\n",
    "    step=state_batch.step,\n",
    "    apply_fn=state.apply_fn,\n",
    "    tx=state.tx,\n",
    "    params=params_update,\n",
    "    opt_state=opt_state_update,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d4c52cb-50ff-4521-b743-3911bcee802d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(561.5625, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43230ff-4b65-4911-b37b-aa317f9a1c16",
   "metadata": {},
   "source": [
    "## Testing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe77b33d-3314-40bf-9010-1bfd922ee580",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "key = jax.random.PRNGKey(314)\n",
    "key, key_params_init, key_eps_init = jax.random.split(key, 3)\n",
    "\n",
    "warmup, test = hlax.datasets.load_fashion_mnist(n_train=1000, n_test=100, normalize=False)\n",
    "X_warmup = warmup[0]\n",
    "\n",
    "dim_latent = 20\n",
    "num_obs, *dim_obs = X_warmup.shape\n",
    "model_vae = hlax.models.UnamortisedVAEBern(dim_latent, dim_obs[0], Encoder, Decoder)\n",
    "\n",
    "# Initialise model consider total number of observations.\n",
    "# We will then slice over the batches\n",
    "batch_init = jnp.ones((num_obs, *dim_obs))\n",
    "params = model_vae.init(key_params_init, batch_init, key_eps_init, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327a5a21-2d02-4076-912f-a989fe0aca37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cac0be8832441d2ae3224c3b61ef631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1801633/1852132205.py\u001b[0m(14)\u001b[0;36miwae_bern\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m    \u001b[0mencode_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m    \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_mean_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m    \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_is_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "tx = optax.adam(1e-3)\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=partial(model_vae.apply, num_samples=num_samples),\n",
    "    params=params,\n",
    "    tx=tx,\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "keys = jax.random.split(key, num_epochs)\n",
    "\n",
    "losses = []\n",
    "for key_epoch in (pbar := tqdm(keys)):\n",
    "    loss, state = hlax.unamortised.train_epoch(key_epoch, X_warmup, state, batch_size, iwae_bern)\n",
    "    losses.append(loss)\n",
    "    pbar.set_description(f\"loss={loss:0.5e}\")\n",
    "losses = jnp.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5254aa84-0011-4bfa-a75f-4e352f6451f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        decoder: {\n",
       "            Dense_0: {\n",
       "                bias: (20,),\n",
       "                kernel: (20, 20),\n",
       "            },\n",
       "            mean: {\n",
       "                bias: (784,),\n",
       "                kernel: (20, 784),\n",
       "            },\n",
       "        },\n",
       "        encoder: {\n",
       "            Dense_0: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 784, 100),\n",
       "            },\n",
       "            Dense_1: {\n",
       "                bias: (1000, 100),\n",
       "                kernel: (1000, 100, 100),\n",
       "            },\n",
       "            Dense_2: {\n",
       "                bias: (1000, 20),\n",
       "                kernel: (1000, 100, 20),\n",
       "            },\n",
       "            Dense_3: {\n",
       "                bias: (1000, 20),\n",
       "                kernel: (1000, 100, 20),\n",
       "            },\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(jnp.shape, state.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
